% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mvInnov.R
\name{mvInnov}
\alias{mvInnov}
\title{Multivariate innovations algorithm (generic covariance backend)}
\usage{
mvInnov(
  u,
  Gamma = NULL,
  Kbig = NULL,
  kappa = NULL,
  jitter = 1e-10,
  symmetrize = TRUE,
  lag_max = NULL,
  lag_tol = 1e-08,
  ahead = 1L,
  PACKAGE = "TSAOP"
)
}
\arguments{
\item{u}{Numeric matrix of dimension \eqn{T \times m}. Rows correspond to time,
columns to components. Typically mean-corrected.}

\item{Gamma}{Optional block-Toeplitz lag covariance array of dimension \code{m x m x L},
where \code{Gamma[,,h+1]} equals \eqn{\Gamma_h = \mathrm{Cov}(u_t, u_{t-h})} for \eqn{h=0,1,\dots,L-1}.
Only one-sided blocks are required. For \eqn{i<j}, the implied covariance is \eqn{K(i,j)=\Gamma_{j-i}^\top}.
Must be long enough for the maximum lag accessed: at least \code{T} if \code{lag_max} is \code{NULL},
otherwise at least \code{lag_max + 1}.}

\item{Kbig}{Optional full covariance matrix for \code{vec(u)} of dimension \eqn{(T m)\times(T m)}.
The block \eqn{K(i,j)} is extracted as rows \code{((i-1)m+1):(im)} and columns \code{((j-1)m+1):(jm)}.}

\item{kappa}{Optional function \code{kappa(i,j)} returning an \code{m x m} numeric matrix \eqn{K(i,j)}.
Indices \code{i,j} are 1-based. If \code{symmetrize=TRUE}, only diagonal blocks \eqn{K(t,t)} are symmetrized.}

\item{jitter}{Nonnegative scalar. A small ridge term is added to each innovation covariance \code{V[[t]]}
(via \code{jitter * I_m}) to improve numerical stability if matrices are nearly singular.
Default \code{1e-10}.}

\item{symmetrize}{Logical. If \code{TRUE} (default), diagonal blocks \eqn{K(t,t)} and innovation covariances
\eqn{V_t} are symmetrized internally as \eqn{(A + A^\top)/2} before Cholesky/inversion steps.
Off-diagonal blocks \eqn{K(i,j)} for \eqn{i\ne j} are not symmetrized.}

\item{lag_max}{Optional integer. If \code{NULL} (default), the algorithm may retain up to \eqn{T-1} lags.
If supplied, \code{lag_max} is a \strong{hard cap} on the number of lag matrices retained/used:
at each time \eqn{t}, \code{Theta[[t]]} has length \code{L_t = min(t-1, lag_max_used)}, where
\code{lag_max_used <= lag_max}. This controls memory and computation.}

\item{lag_tol}{Optional nonnegative scalar. If not \code{NULL}, enables an \strong{adaptive lag-length rule}.
As the recursion progresses, lag coefficients typically decay toward zero. Once the \strong{oldest retained}
lag coefficient is sufficiently small (i.e., \code{max(abs(Theta[[t]][[L_t]])) < lag_tol}),
the algorithm \strong{fixes} the effective lag length for subsequent times (returns \code{lag_max_used}).
Default \code{1e-8}. Use \code{NULL} to disable tolerance-based lag fixing.}

\item{ahead}{Integer \eqn{h \ge 1}. If not \code{NULL}, also compute the \eqn{h}-step-ahead conditional mean
\eqn{E[u_t \mid u_{1:(t-h)}]} based on the innovations representation. If \code{1L},
\code{uhat_ahead} equals \code{uhat}. If \code{NULL}, \code{uhat_ahead} is omitted.}

\item{PACKAGE}{Character scalar giving the package/DLL name used for registered symbol lookup in \code{.Call()}.
Defaults to \code{"TSAOP"}.}
}
\value{
A named list with components:
\describe{
\item{Theta}{List of length \eqn{T}. Each \code{Theta[[t]]} is a list of length
\code{L_t = min(t-1, lag_max_used)}, where \code{Theta[[t]][[j]]} is an \eqn{m\times m}
coefficient matrix multiplying \code{innov[t-j, ]}.}
\item{V}{List of length \eqn{T}. Each \code{V[[t]]} is an \eqn{m\times m} innovation covariance matrix \eqn{V_t}.}
\item{uhat}{Numeric matrix \eqn{T\times m} of one-step conditional means \eqn{\hat u_t}.}
\item{innov}{Numeric matrix \eqn{T\times m} of one-step innovations \eqn{e_t}.}
\item{uhat_ahead}{If \code{ahead} is not \code{NULL}, numeric matrix \eqn{T\times m} of \eqn{h}-step-ahead means;
otherwise \code{NULL}.}
\item{lag_max_used}{Integer. The final effective lag length used after applying \code{lag_max} and (if enabled)
the tolerance rule \code{lag_tol}.}
\item{lag_used_path}{Integer vector of length \eqn{T}. Entry \code{lag_used_path[t]} equals \code{L_t}, the number
of lag matrices actually used at time \eqn{t}.}
}
}
\description{
Compute the multivariate innovations representation for a zero-mean Gaussian
vector time series \eqn{u_1,\dots,u_T}, where each \eqn{u_t \in \mathbb{R}^m},
given access to the block covariance \eqn{K(i,j) = \mathrm{Cov}(u_i, u_j)}.
}
\details{
The algorithm computes, for each time \eqn{t}, a set of matrix coefficients
\eqn{\Theta_{t,j} \in \mathbb{R}^{m\times m}} and innovation covariance matrices
\eqn{V_t \in \mathbb{R}^{m\times m}} such that
\deqn{\hat u_t = \sum_{j=1}^{L_t} \Theta_{t,j}\, e_{t-j}, \qquad e_t = u_t - \hat u_t,}
where \eqn{e_t} are one-step innovations with covariance \eqn{V_t}, and
\eqn{L_t \le t-1} is the number of lag matrices retained at time \eqn{t}.

This wrapper dispatches to a C implementation via registered \code{.Call()}
routines and supports three interchangeable covariance backends:
\itemize{
\item \strong{Block Toeplitz} via \code{Gamma}: a 3D array \code{m x m x L} storing
one-sided lag covariances \eqn{\Gamma_h = \mathrm{Cov}(u_t, u_{t-h})} for \eqn{h\ge 0}.
This is typically fastest and avoids repeated covariance lookups.
\item \strong{Full covariance} via \code{Kbig}: an explicit \eqn{(Tm)\times(Tm)} covariance matrix
for \code{vec(u)}. This is general but can be large in memory.
\item \strong{Callback} via \code{kappa(i,j)}: an R function returning an \code{m x m} block
\eqn{K(i,j)}. This is most flexible but can be much slower due to R-to-C calls.
}

\strong{Gaussian likelihood (up to a constant).}
If you want a negative log-likelihood contribution from the innovations output,
a common form is
\deqn{\frac12 \sum_{t=1}^T \left( \log |V_t| + e_t^\top V_t^{-1} e_t \right),}
where \eqn{e_t} are the rows of \code{innov} and \eqn{V_t} are the matrices in \code{V}.

\strong{Performance.}
The \code{Gamma} backend is typically fastest. The \code{kappa} callback can be substantially slower
for large \eqn{T} because it requires many calls back into R.
}
\examples{
# Example: VAR(1) with block Toeplitz backend (Gamma)
set.seed(1)
Tt <- 200
A <- matrix(c(0.6, 0.1,
              0.0, 0.4), 2, 2, byrow = TRUE)
Sigma <- matrix(c(1.0, 0.3,
                  0.3, 0.8), 2, 2)

simulate_var1 <- function(T, A, Sigma, burn = 200) {
  A <- as.matrix(A); Sigma <- as.matrix(Sigma)
  m <- nrow(A)
  U <- matrix(0, T + burn, m)
  Z <- matrix(rnorm((T + burn) * m), T + burn, m)
  E <- Z \%*\% chol(Sigma)
  for (t in 2:(T + burn)) U[t, ] <- A \%*\% U[t - 1, ] + E[t, ]
  U[(burn + 1):(T + burn), , drop = FALSE]
}

Gamma_var1 <- function(A, Sigma, L) {
  A <- as.matrix(A); Sigma <- as.matrix(Sigma)
  m <- nrow(A)
  M <- diag(m*m) - kronecker(A, A)
  vecG0 <- solve(M, as.vector(Sigma))
  G0 <- matrix(vecG0, m, m)

  Gamma <- array(0, dim = c(m, m, L))
  Gamma[, , 1] <- G0
  Ah <- diag(m)
  for (h in 1:(L - 1L)) {
    Ah <- Ah \%*\% A
    Gamma[, , h + 1L] <- Ah \%*\% G0
  }
  Gamma
}

u <- simulate_var1(Tt, A, Sigma)
u <- scale(u, center = TRUE, scale = FALSE)
Gamma <- Gamma_var1(A, Sigma, L = Tt)

fit <- mvInnov(u, Gamma = Gamma, lag_max = NULL, lag_tol = NULL, ahead = 2L)
str(fit, max.level = 2)

}
